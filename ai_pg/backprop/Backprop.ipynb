{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjuntos de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND\n",
    "\n",
    "Resultados respectivos a cada entrada armazenada na lista de combinações após a aplicação da operação lógica `AND`. \n",
    "\n",
    "| x | y | z |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 |\n",
    "| 1 | 0 | 0 |\n",
    "| 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "and_targets = np.array([[0], [0], [0], [1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR\n",
    "\n",
    "Resultados respectivos a cada entrada armazenada na lista de combinações após a aplicação da operação lógica `OR`. \n",
    "\n",
    "| x | y | z |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "or_targets = np.array([[0], [1], [1], [1]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR\n",
    "\n",
    "Resultados respectivos a cada entrada armazenada na lista de combinações após a aplicação da operação lógica `XOR`. \n",
    "\n",
    "| x | y | z |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 0 | 1 |\n",
    "| 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "xor_targets = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rede Neural\n",
    "\n",
    "A implementação da [Rede Neural](https://en.wikipedia.org/wiki/Artificial_neural_network) especificada abaixo tem como base o modelo de perceptron multicamadas e enfoque na classificação por retropropagação com a opção de utilização de duas funções para o cálculo da saída de determinado neurônio, sendo elas a  **sigmoide** e a **hiperbólica**.\n",
    "\n",
    "Tanto o procedimento de inicialização quanto o de execução utilizados seguem o processo especificado em sala de aula:\n",
    "\n",
    "1. Inicialmente os pesos sinápticos de entrada e saída das camadas de neurônios ocultos são determinados aleatoriamente.\n",
    "\n",
    "2. O valor da ativação é calculado a partir do produto vetorial dos vetores de entradas e de pesos.\n",
    "\n",
    "3. O valor de saída é determinado a partir da função escolhida (a versão atual utiliza a função hiperbólica).\n",
    "\n",
    "4. O erro da camada de saída é calculado a partir da fórmula:\n",
    "\n",
    "    ![image.png](attachment:image.png)\n",
    "\n",
    "    Como especificado em\n",
    "    \n",
    "    ```python\n",
    "        out_err = target - final_out\n",
    "        out_delta = out_err * self.tanh_deriv(final_out)\n",
    "    ```\n",
    "\n",
    "    E o erro das camadas ocultas é calculado por:\n",
    "\n",
    "    ```python\n",
    "        h_err = np.dot(out_delta, self.out_w.T)\n",
    "        h_delta = h_err * self.tanh_deriv(h_out)\n",
    "    ```\n",
    "\n",
    "5. O ajuste dos pesos é calculado com base nos erros identificados em determinada iteração:\n",
    "    \n",
    "    ```python\n",
    "        self.in_w += learning_rate * np.outer(inputs, h_delta)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, isize=2, hsize=4, osize=1):\n",
    "        self.isize = isize\n",
    "        self.hsize = hsize\n",
    "        self.osize = osize\n",
    "\n",
    "        # Weight initialization\n",
    "        self.in_w = np.random.rand(self.isize, self.hsize)\n",
    "        self.out_w = np.random.rand(self.hsize, self.osize)\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_deriv(self, x):\n",
    "        return x * (1 - x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def tanh_deriv(self, x):\n",
    "        return 1.0 - np.tanh(x) ** 2\n",
    "\n",
    "    def propagate(self, inputs):\n",
    "        h_sum = np.dot(inputs, self.in_w)\n",
    "        h_out = self.tanh(h_sum)\n",
    "\n",
    "        # Output layer\n",
    "        out_sum = np.dot(h_out, self.out_w)\n",
    "        final_out = self.tanh(out_sum)\n",
    "\n",
    "        return h_out, final_out\n",
    "\n",
    "    def backpropagate(self, inputs, target, h_out, final_out, learning_rate):\n",
    "        out_err = target - final_out\n",
    "        out_delta = out_err * self.tanh_deriv(final_out)\n",
    "\n",
    "        self.out_w += learning_rate * np.outer(h_out, out_delta)\n",
    "\n",
    "        # Errors that happened in the hidden layers\n",
    "        h_err = np.dot(out_delta, self.out_w.T)\n",
    "        h_delta = h_err * self.tanh_deriv(h_out)\n",
    "\n",
    "        # Update hidden layers' weights\n",
    "        self.in_w += learning_rate * np.outer(inputs, h_delta)\n",
    "\n",
    "    def train(self, inputs, targets, iterations=5000, learning_rate=.08):\n",
    "        for iter in range(iterations):\n",
    "            # Total error\n",
    "            err = 0\n",
    "            for i in range(len(inputs)):\n",
    "                input_data = inputs[i]\n",
    "                target_data = targets[i]\n",
    "\n",
    "                # Propagate\n",
    "                h_out, final_out = self.propagate(input_data)\n",
    "\n",
    "                # Backpropagate\n",
    "                self.backpropagate(input_data, target_data, h_out, final_out, learning_rate)\n",
    "\n",
    "                err += np.sum((target_data - final_out) ** 2)\n",
    "\n",
    "            if iter % 1000 == 0:\n",
    "                print(f\"Iter: {iter}\\t Error: {err / len(inputs)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 2\n",
    "hidden_size = 6\n",
    "output_size = 1\n",
    "learning_rate = 0.1\n",
    "iterations = 10000"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\t Error: 0.4481788024485403\n",
      "Iter: 1000\t Error: 5.877233860710746e-05\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 2000\t Error: 9.467566301039427e-06\n",
      "Iter: 3000\t Error: 3.4829755472410718e-06\n",
      "Iter: 4000\t Error: 1.7488193432981806e-06\n",
      "Iter: 5000\t Error: 1.0344436248333655e-06\n",
      "Iter: 6000\t Error: 6.770341270346617e-07\n",
      "Iter: 7000\t Error: 4.746074727321206e-07\n",
      "Iter: 8000\t Error: 3.4963440211109543e-07\n",
      "Iter: 9000\t Error: 2.6742193085221696e-07\n"
     ]
    }
   ],
   "source": [
    "neural_net_and = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "neural_net_and.train(inputs, and_targets, iterations, learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\t Error: 0.039393005944754304\n",
      "Iter: 1000\t Error: 1.7406559048616257e-06\n",
      "Iter: 2000\t Error: 3.82180438259308e-07\n",
      "Iter: 3000\t Error: 1.5794066955263732e-07\n",
      "Iter: 4000\t Error: 8.453467270443677e-08\n",
      "Iter: 5000\t Error: 5.2118157714579974e-08\n",
      "Iter: 6000\t Error: 3.513272611267727e-08\n",
      "Iter: 7000\t Error: 2.5184983669007372e-08\n",
      "Iter: 8000\t Error: 1.888378813208689e-08\n",
      "Iter: 9000\t Error: 1.4652881389241122e-08\n"
     ]
    }
   ],
   "source": [
    "neural_net_or = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "neural_net_or.train(inputs, or_targets, iterations, learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\t Error: 0.25112666759264846\n",
      "Iter: 1000\t Error: 0.00014721813663351083\n",
      "Iter: 2000\t Error: 2.1805424971581473e-05\n",
      "Iter: 3000\t Error: 7.951387789501374e-06\n",
      "Iter: 4000\t Error: 3.998632787500239e-06\n",
      "Iter: 5000\t Error: 2.374332659404816e-06\n",
      "Iter: 6000\t Error: 1.560637597191495e-06\n",
      "Iter: 7000\t Error: 1.0986033734621846e-06\n",
      "Iter: 8000\t Error: 8.125040815690101e-07\n",
      "Iter: 9000\t Error: 6.237189798109872e-07\n"
     ]
    }
   ],
   "source": [
    "neural_net_xor = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "neural_net_xor.train(inputs, xor_targets, iterations, learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Teste"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Output: [0.]\n",
      "Input: [0 1], Output: [0.00026171]\n",
      "Input: [1 0], Output: [0.00026457]\n",
      "Input: [1 1], Output: [0.9991674]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs)):\n",
    "    input = inputs[i]\n",
    "    _, out = neural_net_and.propagate(input)\n",
    "    print(f\"Input: {input}, Output: {out}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Output: [0.]\n",
      "Input: [0 1], Output: [0.9998265]\n",
      "Input: [1 0], Output: [0.99987118]\n",
      "Input: [1 1], Output: [0.99999459]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs)):\n",
    "    input = inputs[i]\n",
    "    _, out = neural_net_or.propagate(input)\n",
    "    print(f\"Input: {input}, Output: {out}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [0 0], Output: [0.]\n",
      "Input: [0 1], Output: [0.99904763]\n",
      "Input: [1 0], Output: [0.99905864]\n",
      "Input: [1 1], Output: [1.34974669e-05]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(inputs)):\n",
    "    input = inputs[i]\n",
    "    _, out = neural_net_xor.propagate(input)\n",
    "    print(f\"Input: {input}, Output: {out}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questão 1 (Conclusões)\n",
    "\n",
    "Uma análise comparativa foi realizada considereando os atributos cuja especificação pode ser definida arbitrariamente, tais como a quantidade de camadas ocultas a serem utilizadas pelo modelo, a quantidade de saídas suportadas pelo algoritmo, a taxa de aprendizado, o número de iterações de propagação e a função de ativação.\n",
    "\n",
    "A alteração mais significativa quando comparados os resultados se mostrou no uso da função hiperbólica em contraste à função sigmoide, no sentido em que os resultados apresentados após o uso da primeira possuíram uma acurácia consideravelmente maior que a segunda (uma ordem de magnitude). Essa observação é de extrema importância uma vez que revela o quão sensível as classificações são frente à escolha e experimentação de funções de ativação. O segundo maior fator de mudança se deu na utilização de diferentes taxas de aprendizado e o fenômeno observado é descrito abaixo:\n",
    "\n",
    "- Para uma taxa de aprendizado suficientemente baixa (< 0.1), as classificações geraram resultados insatisfatórios principalmente para as operações `AND` e `XOR` sobre todas as entradas exceto as variáveis [0,0], que foram sempre classificadas corretamente independentemente de qualquer alteração.\n",
    "\n",
    "- Taxas situadas em um intervalo entre 0.3 e 0.4 apresentaram resultados satisfatórios para ambas as operações `AND` e `OR`. Todavia, com um aumento dos valores nesse intervalo, a classificação da operação `XOR` se tornou cada vez menos efetiva.\n",
    "\n",
    "- Taxas de aprendizado no intervalo 0.1 a 0.2 apresentaram os melhores resultados gerais de classificação. Porém, é importante apontar que os melhores valores de classificação das operações `AND` e `OR` não foram resultados do uso de uma taxa nesse intervalo, e sim do uso dos intervalos mencionados acima. A relevância desse intervalo é dada pelo seu maior balanceamento nos resultados de classificação, já que nele a operação `XOR` foi classificada com as maiores taxas de acerto. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
